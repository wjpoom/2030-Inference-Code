# SliMM-Inference: Multi-Framework Inference for SliMM Models

æœ¬é¡¹ç›®æä¾›äº†å°† PyTorch ç‰ˆæœ¬çš„ SliMM æ¨¡å‹è½¬æ¢ä¸º ONNX æ ¼å¼ï¼Œå¹¶è¿›ä¸€æ­¥è½¬æ¢ä¸º MindSporeã€PaddlePaddle å’Œ JAX ä¸‰ç§æ¡†æ¶çš„èƒ½åŠ›ï¼ŒåŒæ—¶æ”¯æŒåœ¨å››ç§æ¡†æ¶ï¼ˆPyTorchã€ONNX Runtimeã€MindSporeã€PaddlePaddleã€JAXï¼‰ä¸Šè¿›è¡Œæ¨ç†ã€‚

## ğŸŒŸ æ ¸å¿ƒåŠŸèƒ½

- **PyTorch â†’ ONNX**: å°† PyTorch æ¨¡å‹å¯¼å‡ºä¸º ONNX æ ¼å¼
- **ONNX â†’ å¤šæ¡†æ¶**: æ”¯æŒå°† ONNX æ¨¡å‹è½¬æ¢ä¸º MindSporeã€PaddlePaddleã€JAX ä¸‰ç§æ¡†æ¶
- **å¤šæ¡†æ¶æ¨ç†**: æ”¯æŒåœ¨ PyTorchã€ONNX Runtimeã€MindSporeã€PaddlePaddleã€JAX äº”ç§æ¡†æ¶ä¸Šè¿›è¡Œæ¨ç†
- **ONNX Runtime åŒ…è£…å™¨**: æä¾›ç»Ÿä¸€çš„åŒ…è£…å™¨æ¥å£ï¼Œç®€åŒ–è·¨æ¡†æ¶ä½¿ç”¨

## ğŸ“‹ ç›®å½•ç»“æ„

```
SliMM-Inference/
â”œâ”€â”€ export_slimm.py              # PyTorch æ¨¡å‹å¯¼å‡ºä¸º ONNX
â”œâ”€â”€ convert_onnx_to_frameworks.py # ONNX è½¬æ¢ä¸ºå…¶ä»–æ¡†æ¶
â”œâ”€â”€ convert_models.py            # æ¨¡å‹è½¬æ¢å·¥å…·
â”œâ”€â”€ onnx_wrappers.py             # ONNX Runtime åŒ…è£…å™¨
â”œâ”€â”€ inference_torch.py           # PyTorch æ¨ç†è„šæœ¬
â”œâ”€â”€ inference_onnx.py            # ONNX Runtime æ¨ç†è„šæœ¬
â”œâ”€â”€ inference_mindspore.py       # MindSpore æ¨ç†è„šæœ¬
â”œâ”€â”€ inference_paddlepaddle.py    # PaddlePaddle æ¨ç†è„šæœ¬
â”œâ”€â”€ inference_jax.py             # JAX æ¨ç†è„šæœ¬
â”œâ”€â”€ CONVERSION_README.md         # è½¬æ¢è¯¦ç»†è¯´æ˜
â””â”€â”€ slimm/                       # SliMM æ¨¡å‹ä»£ç 
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
# åˆ›å»º conda ç¯å¢ƒ
conda create -n slimm python=3.10 -y
conda activate slimm

# å®‰è£…åŸºç¡€ä¾èµ–
pip install --upgrade pip
pip install -e .

# å®‰è£… transformersï¼ˆéœ€è¦ç‰¹å®šç‰ˆæœ¬ï¼‰
pip install transformers@git+https://github.com/huggingface/transformers.git@7bbc62474391aff64f63fcc064c975752d1fa4de

# å®‰è£… ONNX Runtimeï¼ˆå¿…éœ€ï¼‰
pip install onnxruntime

# å¯é€‰ï¼šå®‰è£…å…¶ä»–æ¡†æ¶ï¼ˆæ ¹æ®éœ€è¦é€‰æ‹©ï¼‰
# MindSpore
pip install mindspore

# PaddlePaddle
pip install paddlepaddle

# JAX
pip install jax jaxlib
```

### 2. æ¨¡å‹è½¬æ¢æµç¨‹

#### æ­¥éª¤ 1: PyTorch â†’ ONNX

é¦–å…ˆå°† PyTorch æ¨¡å‹å¯¼å‡ºä¸º ONNX æ ¼å¼ï¼š

```bash
# ç¼–è¾‘ export_slimm.pyï¼Œè®¾ç½®æ¨¡å‹è·¯å¾„
# path = '/path/to/SliMM-Qwen2-0.5B'
# onnx_model_A = '/path/to/slimm_onnx/SliMM_A.onnx'
# ... è®¾ç½®å…¶ä»–æ¨¡å‹è·¯å¾„

python export_slimm.py
```

è¿™å°†ç”Ÿæˆä»¥ä¸‹ ONNX æ¨¡å‹æ–‡ä»¶ï¼š
- `SliMM_A.onnx` - æ–‡æœ¬ç¼–ç å™¨
- `SliMM_B.onnx` - è§†è§‰ç¼–ç å™¨
- `SliMM_C.onnx` - å¤šæ¨¡æ€èåˆ
- `SliMM_D.onnx` - Rotary ä½ç½®ç¼–ç ï¼ˆæœ‰è§†è§‰ï¼‰
- `SliMM_E.onnx` - Rotary ä½ç½®ç¼–ç ï¼ˆæ— è§†è§‰ï¼‰
- `SliMM_F.onnx` - LLM è§£ç å™¨

#### æ­¥éª¤ 2: ONNX â†’ å…¶ä»–æ¡†æ¶

ä½¿ç”¨ `convert_onnx_to_frameworks.py` è„šæœ¬è¿›è¡Œè½¬æ¢ï¼š

```bash
# ç¼–è¾‘è„šæœ¬ä¸­çš„è·¯å¾„é…ç½®
python convert_onnx_to_frameworks.py
```

è¯¦ç»†è¯´æ˜è¯·å‚è€ƒ [CONVERSION_README.md](CONVERSION_README.md)ã€‚

### 3. è¿è¡Œæ¨ç†

è½¬æ¢å®Œæˆåï¼Œå¯ä»¥ä½¿ç”¨ç›¸åº”çš„æ¨ç†è„šæœ¬è¿›è¡Œæ¨ç†ï¼š

#### PyTorch æ¨ç†

```bash
python inference_torch.py
```

#### ONNX Runtime æ¨ç†

```bash
python inference_onnx.py
```

#### MindSpore æ¨ç†

```bash
python inference_mindspore.py
```

#### PaddlePaddle æ¨ç†

```bash
python inference_paddlepaddle.py
```

#### JAX æ¨ç†

```bash
python inference_jax.py
```

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

### PyTorch æ¨ç†ç¤ºä¾‹

```python
from slimm.model.processor import SliMMQwen2VLProcessor
from slimm.model.slimm import SliMMForConditionalGeneration
from slimm.model.utils_vl import process_vision_info

model_path = "ckpt/SliMM-DeepStackE-Qwen2VL-2B"

model = SliMMForConditionalGeneration.from_pretrained(
    model_path, torch_dtype="auto", device_map="cuda:0"
)

processor = SliMMQwen2VLProcessor.from_pretrained(model_path)

messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "demo.jpeg"},
            {"type": "text", "text": "Describe this image."},
        ],
    }
]

# å‡†å¤‡è¾“å…¥
text = processor.apply_chat_template(
    messages, tokenize=False, add_generation_prompt=True
)
image_inputs, video_inputs = process_vision_info(messages)
inputs = processor(
    text=[text],
    images=image_inputs,
    videos=video_inputs,
    padding=True,
    return_tensors="pt",
)
inputs = inputs.to("cuda")

# æ¨ç†
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
```
### æ¨ç†å‚æ•°é…ç½®

```python
INPUT_IMAGE_SIZE = [960, 960]      # è¾“å…¥å›¾åƒå°ºå¯¸
HEIGHT_FACTOR = 15                 # é«˜åº¦å› å­
WIDTH_FACTOR = 15                  # å®½åº¦å› å­
MAX_SEQ_LENGTH = 4096              # æœ€å¤§åºåˆ—é•¿åº¦
STOP_TOKEN = [151643, 151645]      # åœæ­¢ token
```

## ğŸ› ï¸ æŠ€æœ¯ç»†èŠ‚

### ONNX Runtime åŒ…è£…å™¨

åŒ…è£…å™¨ç±»æä¾›äº†ç»Ÿä¸€çš„æ¥å£ï¼Œä½¿å¾—åœ¨ä¸åŒæ¡†æ¶ä¸­ä½¿ç”¨ ONNX æ¨¡å‹å˜å¾—ç®€å•ï¼š

- **MindSporeWrapper**: å°† ONNX è¾“å‡ºè½¬æ¢ä¸º MindSpore Tensor
- **PaddlePaddleWrapper**: å°† ONNX è¾“å‡ºè½¬æ¢ä¸º PaddlePaddle Tensor
- **JAXWrapper**: å°† ONNX è¾“å‡ºè½¬æ¢ä¸º JAX Array

æ‰€æœ‰åŒ…è£…å™¨éƒ½æ”¯æŒï¼š
- è‡ªåŠ¨ç±»å‹è½¬æ¢
- GPU/CPU è‡ªåŠ¨é€‰æ‹©
- è¾“å…¥è¾“å‡ºåç§°è‡ªåŠ¨è·å–
- åºåˆ—åŒ–æ”¯æŒï¼ˆå¯ä¿å­˜ä¸º .pkl æ–‡ä»¶ï¼‰

### æ¨¡å‹åˆ†å‰²ç­–ç•¥

SliMM æ¨¡å‹è¢«åˆ†å‰²ä¸º 6 ä¸ªå­æ¨¡å‹ï¼š

1. **SliMM_A**: æ–‡æœ¬ç¼–ç å™¨ï¼ˆEmbedding + éƒ¨åˆ† Transformerï¼‰
2. **SliMM_B**: è§†è§‰ç¼–ç å™¨ï¼ˆVision Encoderï¼‰
3. **SliMM_C**: å¤šæ¨¡æ€èåˆå±‚
4. **SliMM_D**: Rotary ä½ç½®ç¼–ç ï¼ˆæœ‰è§†è§‰è¾“å…¥æ—¶ä½¿ç”¨ï¼‰
5. **SliMM_E**: Rotary ä½ç½®ç¼–ç ï¼ˆæ— è§†è§‰è¾“å…¥æ—¶ä½¿ç”¨ï¼‰
6. **SliMM_F**: LLM è§£ç å™¨ï¼ˆä¸»è¦ Transformer å±‚ï¼‰

è¿™ç§åˆ†å‰²ç­–ç•¥ä½¿å¾—ï¼š
- å¯ä»¥çµæ´»ç»„åˆä¸åŒçš„å­æ¨¡å‹
- æ”¯æŒå¢é‡æ¨ç†ï¼ˆKV Cacheï¼‰
- ä¾¿äºåœ¨ä¸åŒæ¡†æ¶ä¸­å®ç°

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **ä¾èµ–ç‰ˆæœ¬**: ç¡®ä¿ä½¿ç”¨æ­£ç¡®ç‰ˆæœ¬çš„ transformersï¼ˆcommit 7bbc6247ï¼‰
2. **æ¨¡å‹è·¯å¾„**: ç¡®ä¿æ‰€æœ‰æ¨¡å‹è·¯å¾„é…ç½®æ­£ç¡®
3. **æ¡†æ¶å…¼å®¹æ€§**: ä¸åŒæ¡†æ¶çš„ tensor ç±»å‹å¯èƒ½ä¸åŒï¼ŒåŒ…è£…å™¨ä¼šè‡ªåŠ¨å¤„ç†
4. **GPU æ”¯æŒ**: å¦‚éœ€ä½¿ç”¨ GPUï¼Œç¡®ä¿å®‰è£…äº†ç›¸åº”çš„ CUDA ç‰ˆæœ¬å’Œæ¡†æ¶ GPU ç‰ˆæœ¬
5. **å†…å­˜ç®¡ç†**: å¤§æ¨¡å‹æ¨ç†æ—¶æ³¨æ„å†…å­˜ä½¿ç”¨ï¼Œå¿…è¦æ—¶ä½¿ç”¨ CPU æ¨ç†

## ğŸ› æ•…éšœæ’é™¤

### è½¬æ¢å¤±è´¥

- æ£€æŸ¥ ONNX æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
- ç¡®ä¿å·²å®‰è£…æ‰€éœ€çš„ä¾èµ–åŒ…
- æŸ¥çœ‹é”™è¯¯ä¿¡æ¯ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´è¾“å…¥/è¾“å‡ºèŠ‚ç‚¹åç§°

### æ¨ç†å¤±è´¥

- ç¡®ä¿å·²åˆ›å»ºåŒ…è£…å™¨æ–‡ä»¶ï¼ˆ.pklï¼‰
- æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦æ­£ç¡®
- ç¡®ä¿å·²å®‰è£…ç›¸åº”æ¡†æ¶
- æ£€æŸ¥è¾“å…¥/è¾“å‡ºå½¢çŠ¶æ˜¯å¦åŒ¹é…
- æŸ¥çœ‹æ¡†æ¶ç‰¹å®šçš„é”™è¯¯ä¿¡æ¯

### æ€§èƒ½é—®é¢˜

- ONNX Runtime åŒ…è£…å™¨æ€§èƒ½å¯èƒ½ç•¥ä½äºåŸç”Ÿæ¡†æ¶
- å¦‚éœ€æœ€ä½³æ€§èƒ½ï¼Œå»ºè®®ä½¿ç”¨åŸç”Ÿæ¡†æ¶è½¬æ¢
- GPU æ¨ç†é€šå¸¸æ¯” CPU å¿«å¾ˆå¤š

## ğŸ“š ç›¸å…³èµ„æº

- [SliMM é¡¹ç›®ä¸»é¡µ](https://deepstack-vl.github.io/)
- [SliMM è®ºæ–‡](https://arxiv.org/abs/2406.04334)
- [HuggingFace æ¨¡å‹](https://huggingface.co/collections/menglc/slimm-675bd737c2965037a6b52d05)
- [ONNX Runtime æ–‡æ¡£](https://onnxruntime.ai/)
- [MindSpore æ–‡æ¡£](https://www.mindspore.cn/)
- [PaddlePaddle æ–‡æ¡£](https://www.paddlepaddle.org.cn/)
- [JAX æ–‡æ¡£](https://jax.readthedocs.io/)

## ğŸ“„ è®¸å¯è¯

è¯·æŸ¥çœ‹ [LICENSE](LICENSE) æ–‡ä»¶äº†è§£è¯¦æƒ…ã€‚

## ğŸ™ è‡´è°¢

æœ¬é¡¹ç›®åŸºäºä»¥ä¸‹ä¼˜ç§€é¡¹ç›®ï¼š
- [Qwen2-VL](https://github.com/QwenLM/Qwen2-VL)
- [LLaVA](https://github.com/haotian-liu/LLaVA)
- [LLaVA-NeXT](https://github.com/LLaVA-VL/LLaVA-NeXT)

## ğŸ“§ è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·æäº¤ Issue æˆ– Pull Requestã€‚

---

**æ³¨æ„**: æœ¬é¡¹ç›®ä¸“æ³¨äºæ¨ç†åŠŸèƒ½ï¼Œè®­ç»ƒç›¸å…³ä»£ç è¯·å‚è€ƒä¸» SliMM é¡¹ç›®ã€‚
